{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtr1.shape (10000, 3072)\n",
      "ylab1.shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "#neature network\n",
    "import os\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "\n",
    "#数据读取\n",
    "def data_read(data_path):\n",
    "    with open(data_path, 'rb') as dataSet:\n",
    "        data_dict = pk.load(dataSet, encoding = 'latin1')\n",
    "        #print(data_dict)\n",
    "        xdata = data_dict['data']\n",
    "        ydata = data_dict['labels']\n",
    "           \n",
    "        X_data = xdata.reshape(10000, 3*32*32)\n",
    "        Y_label = np.array(ydata)\n",
    "        dataSet.close()\n",
    "    \n",
    "        return X_data, Y_label\n",
    " \n",
    "UbuntuBaseDir = '/media/cui/AC86F58E86F55972'\n",
    "WinBaseDir = 'E:'\n",
    "BaseDir = UbuntuBaseDir\n",
    "#分别读取五部分的数据做价差验证，然后取平均值，得到最好的K和距离范式后再跑测试集\n",
    "datadir1 = os.path.join(BaseDir, 'seedclass/机器学习/cs231/cifar-10-batches-py/data_batch_1')\n",
    "datadir2 = os.path.join(BaseDir, 'seedclass/机器学习/cs231/cifar-10-batches-py/data_batch_2')\n",
    "datadir3 = os.path.join(BaseDir, 'seedclass/机器学习/cs231/cifar-10-batches-py/data_batch_3')\n",
    "datadir4 = os.path.join(BaseDir, 'seedclass/机器学习/cs231/cifar-10-batches-py/data_batch_4')\n",
    "datadir5 = os.path.join(BaseDir, 'seedclass/机器学习/cs231/cifar-10-batches-py/data_batch_5')\n",
    "\n",
    "xtr1, ylab1 = data_read(datadir1)\n",
    "xtr2, ylab2 = data_read(datadir2)\n",
    "xtr3, ylab3 = data_read(datadir3)\n",
    "xtr4, ylab4 = data_read(datadir4)\n",
    "xtr5, ylab5 = data_read(datadir5)\n",
    "print('xtr1.shape',xtr1.shape)\n",
    "print('ylab1.shape',ylab1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def add_layer(inputs, in_size, out_size, activation_function=None): \n",
    "# add one more layer and return the output of this layer \n",
    "    Weights = tf.Variable(tf.random_normal([in_size, out_size])) \n",
    "    biases = tf.Variable(tf.zeros([1, out_size]) + 0.1) \n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases \n",
    "    if activation_function is None: \n",
    "        outputs = Wx_plus_b \n",
    "    else: outputs = activation_function(Wx_plus_b) \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 3072)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.vstack((xtr1, xtr2))\n",
    "X_train = np.vstack((X_train, xtr3))\n",
    "X_train = np.vstack((X_train, xtr4))\n",
    "Y_train = np.hstack((ylab1, ylab2))\n",
    "Y_train = np.hstack((Y_train, ylab3))\n",
    "Y_train = np.hstack((Y_train, ylab4))\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#定义添加隐含层的函数\n",
    "def add_layer(inputs, in_size, out_size,keep_prob=1.0,activation_function=None):\n",
    "    Weights = tf.Variable(tf.truncated_normal([in_size,out_size],stddev=0.1))\n",
    "    biases = tf.Variable(tf.zeros([out_size]))\n",
    "    Wx_plus_b = tf.matmul(inputs, Weights) + biases\n",
    "    if activation_function is None:\n",
    "        outputs = Wx_plus_b\n",
    "    else:\n",
    "        outputs = activation_function(Wx_plus_b)\n",
    "        outputs = tf.nn.dropout(outputs,keep_prob)  #随机失活\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mean_11:0\", shape=(), dtype=float32)\n",
      "step 0,train_accuracy 1.000000\n",
      "step 1000,train_accuracy 1.000000\n",
      "step 2000,train_accuracy 1.000000\n",
      "step 3000,train_accuracy 1.000000\n",
      "step 4000,train_accuracy 1.000000\n",
      "step 5000,train_accuracy 1.000000\n",
      "step 6000,train_accuracy 1.000000\n",
      "step 7000,train_accuracy 1.000000\n",
      "step 8000,train_accuracy 1.000000\n",
      "step 9000,train_accuracy 1.000000\n",
      "step 10000,train_accuracy 1.000000\n",
      "step 11000,train_accuracy 1.000000\n",
      "step 12000,train_accuracy 1.000000\n",
      "step 13000,train_accuracy 1.000000\n",
      "step 14000,train_accuracy 1.000000\n",
      "step 15000,train_accuracy 1.000000\n",
      "step 16000,train_accuracy 1.000000\n",
      "step 17000,train_accuracy 1.000000\n",
      "step 18000,train_accuracy 1.000000\n",
      "step 19000,train_accuracy 1.000000\n",
      "step 20000,train_accuracy 1.000000\n",
      "step 21000,train_accuracy 1.000000\n",
      "step 22000,train_accuracy 1.000000\n",
      "step 23000,train_accuracy 1.000000\n",
      "step 24000,train_accuracy 1.000000\n",
      "step 25000,train_accuracy 1.000000\n",
      "step 26000,train_accuracy 1.000000\n",
      "step 27000,train_accuracy 1.000000\n",
      "step 28000,train_accuracy 1.000000\n",
      "step 29000,train_accuracy 1.000000\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# holder变量\n",
    "x = tf.placeholder(tf.float32,[None,3072])\n",
    "y_ = tf.placeholder(tf.float32,[None,1])\n",
    "keep_prob = tf.placeholder(tf.float32)     # 概率\n",
    "\n",
    "h1 = add_layer(x,3072,300,keep_prob,tf.nn.relu)\n",
    "##输出层\n",
    "w = tf.Variable(tf.zeros([300,10]))     #300*10\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(h1,w)+b)\n",
    "\n",
    "#定义loss,optimizer\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[1]))\n",
    "train_step  =tf.train.AdagradOptimizer(0.35).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))       #高维度的\n",
    "acuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))    #要用reduce_mean\n",
    "print(acuracy)\n",
    "tf.global_variables_initializer().run()\n",
    "for i in range(0,30000,100):\n",
    "    batch_x = X_train[i:i+100, :]\n",
    "    batch_y = Y_train[i:i+100]\n",
    "    batch_y = batch_y.reshape(100, 1)\n",
    "    #print(batch_x.shape)\n",
    "    #print(batch_y.shape)\n",
    "    \n",
    "    train_step.run({x:batch_x,y_:batch_y,keep_prob:0.05})\n",
    "    if i%1000==0:\n",
    "        train_accuracy = acuracy.eval({x:batch_x,y_:batch_y,keep_prob:1.0})\n",
    "        print(\"step %d,train_accuracy %f\"%(i,train_accuracy))\n",
    "\n",
    "###########test\n",
    "ylab5 = ylab5.reshape(10000, 1)\n",
    "print (acuracy.eval({x:xtr5,y_:ylab5,keep_prob:1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "xs = tf.placeholder(tf.float32, [None, 3072]) \n",
    "ys = tf.placeholder(tf.float32, [None, 1])\n",
    "\n",
    "# 3.定义神经层：隐藏层和预测层 \n",
    "# add hidden layer 输入值是 xs，在隐藏层有 10 个神经元 \n",
    "l1 = add_layer(xs, 3072, 1000, activation_function=tf.nn.relu)\n",
    "# add output layer 输入值是隐藏层 l1，在预测层输出 1 个结果 \n",
    "prediction = add_layer(l1, 1000, 10, activation_function=None) \n",
    "\n",
    "\n",
    "# 4.定义 loss 表达式 \n",
    "# the error between prediciton and real data \n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1])) \n",
    "\n",
    "\n",
    "# 5.选择 optimizer 使 loss 达到最小 \n",
    "# 这一行定义了用什么方式去减少 loss，学习率是 0.1 \n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "# important step 对所有变量进行初始化 \n",
    "init = tf.initialize_all_variables() \n",
    "sess = tf.Session() \n",
    "# 上面定义的都没有运算，直到 sess.run 才会开始运算 \n",
    "sess.run(init)\n",
    "\n",
    "# 迭代 1000 次学习，sess.run optimizer \n",
    "for i in range(1000): \n",
    "    # training train_step 和 loss 都是由 placeholder 定义的运算，所以这里要用 feed 传入参数 \n",
    "    a = X_train[i].reshape(1, 3072)\n",
    "    b = Y_train[i].reshape(1, 1)\n",
    "    sess.run(train_step, feed_dict={xs: a, ys: b})\n",
    "    if i % 50 == 0: \n",
    "        # to see the step improvement\n",
    "        a = xtr5[i].reshape(1, 3072)\n",
    "        b = ylab5[i].reshape(1, 1)\n",
    "        print(sess.run(loss, feed_dict={xs: a, ys: b}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.定义节点准备接收数据 \n",
    "# define placeholder for inputs to network \n",
    "xs = tf.placeholder(tf.float32, [None, 3074]) \n",
    "ys = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)     # 概率\n",
    "\n",
    "h1 = add_layer(x,784,300,keep_prob,tf.nn.relu)\n",
    "##输出层\n",
    "w = tf.Variable(tf.zeros([300,10]))     #300*10\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(h1,w)+b)\n",
    "\n",
    "#定义loss,optimizer\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y),reduction_indices=[1]))\n",
    "train_step = tf.train.AdagradOptimizer(0.35).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))       #高维度的\n",
    "acuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))    #要用reduce_mean\n",
    "\n",
    "tf.global_variables_initializer().run()\n",
    "for i in range(3000):\n",
    "    batch_x,batch_y  = mnist.train.next_batch(100)\n",
    "    train_step.run({x:batch_x,y_:batch_y,keep_prob:0.75})\n",
    "    if i%1000==0:\n",
    "        train_accuracy = acuracy.eval({x:batch_x,y_:batch_y,keep_prob:1.0})\n",
    "        print(\"step %d,train_accuracy %g\"%(i,train_accuracy))\n",
    "\n",
    "###########test\n",
    "\n",
    "print acuracy.eval({x:mnist.test.images,y_:mnist.test.labels,keep_prob:1.0})\n",
    "运行上述程序,得到正确为:0.9784,而这仅仅是加了一个隐含层而已.\n",
    "上面的隐含层的节点加的是300个神经元,如果要再加上一层400个神经元的非常的简单.\n",
    "\n",
    "h1 = add_layer(x,784,300,keep_prob,tf.nn.relu)\n",
    "h2 = add_layer(h1,300,400,keep_prob,tf.nn.relu)\n",
    "##输出层\n",
    "w = tf.Variable(tf.zeros([400,10]))     #300*10\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(h2,w)+b)\n",
    "# 3.定义神经层：隐藏层和预测层 \n",
    "# add hidden layer 输入值是 xs，在隐藏层有 10 个神经元 \n",
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)\n",
    "# add output layer 输入值是隐藏层 l1，在预测层输出 1 个结果 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 1)\n",
      "(300, 1)\n",
      "(300, 1)\n"
     ]
    }
   ],
   "source": [
    "x_data = np.linspace(-1,1,300)[:, np.newaxis]\n",
    "noise = np.random.normal(0, 0.05, x_data.shape)\n",
    "y_data = np.square(x_data) - 0.5 + noise\n",
    "print(x_data.shape)\n",
    "print(noise.shape)\n",
    "print(y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#超参数的选择,神经网络的层数N,激活函数的选择,\n",
    "for i in range(2):\n",
    "    for j in range(2,6):\n",
    "       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2.定义节点准备接收数据 \n",
    "# define placeholder for inputs to network \n",
    "xs = tf.placeholder(tf.float32, [None, 1]) \n",
    "ys = tf.placeholder(tf.float32, [None, 1])\n",
    "# 3.定义神经层：隐藏层和预测层 \n",
    "# add hidden layer 输入值是 xs，在隐藏层有 10 个神经元 \n",
    "l1 = add_layer(xs, 1, 10, activation_function=tf.nn.relu)\n",
    "# add output layer 输入值是隐藏层 l1，在预测层输出 1 个结果 \n",
    "prediction = add_layer(l1, 10, 1, activation_function=None) \n",
    "# 4.定义 loss 表达式 \n",
    "# the error between prediciton and real data \n",
    "loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction), reduction_indices=[1])) \n",
    "# 5.选择 optimizer 使 loss 达到最小 \n",
    "# 这一行定义了用什么方式去减少 loss，学习率是 0.1 \n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "# important step 对所有变量进行初始化 \n",
    "init = tf.initialize_all_variables() \n",
    "sess = tf.Session() \n",
    "# 上面定义的都没有运算，直到 sess.run 才会开始运算 \n",
    "sess.run(init)\n",
    "# 迭代 1000 次学习，\n",
    "sess.run optimizer for i in range(1000): \n",
    "    # training train_step 和 loss 都是由 placeholder 定义的运算，所以这里要用 feed 传入参数 \n",
    "    sess.run(train_step, feed_dict={xs: x_data, ys: y_data}) if i % 50 == 0: \n",
    "        # to see the step improvement \n",
    "        print(sess.run(loss, feed_dict={xs: x_data, ys: y_data}))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
